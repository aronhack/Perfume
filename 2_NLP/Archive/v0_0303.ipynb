{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER\n",
    "from ckiptagger import construct_dictionary\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "host = 4\n",
    "# host = 0\n",
    "\n",
    "if host == 0:\n",
    "    path = r'/Users/aron/Documents/GitHub/Perfume/2_NLP'\n",
    "    dict_file = r'/Users/aron/Documents/GitHub/Perfume/1_Crawler/Resource/dict.xlsx'\n",
    "    data_file = r'/Users/aron/Documents/GitHub/Perfume/1_Crawler/Resource/data.xls'\n",
    "    \n",
    "elif host == 4:\n",
    "    path = r'/home/rserver/Data_Mining/personal_workspace/yz/Lab/CkipTagger/'\n",
    "    dict_file = r'/home/rserver/Data_Mining/personal_workspace/yz/Lab/CkipTagger/Resource/dict.xlsx'\n",
    "    data_file = r'/home/rserver/Data_Mining/personal_workspace/yz/Lab/CkipTagger/Resource/data.xls'\n",
    "\n",
    "    \n",
    "# Codebase ......\n",
    "path_codebase = [r'/Users/aron/Documents/GitHub/Arsenal/',\n",
    "                 r'/Users/aron/Documents/GitHub/Codebase_YZ/',\n",
    "                 r'/home/aronhack/stock_predict/Function',\n",
    "                 r'D:\\GitHub\\Arsenal',\n",
    "                 r'D:\\Data_Mining\\Projects\\Codebase_YZ',\n",
    "                 r'/home/jupyter/Arsenal/20220522',\n",
    "                 path + '/Function']\n",
    "\n",
    "for i in path_codebase:    \n",
    "    if i not in sys.path:\n",
    "        sys.path = [i] + sys.path\n",
    "    \n",
    "import codebase_yz as cbyz\n",
    "    \n",
    "path_resource = path + '/Resource'\n",
    "path_export = path + '/Export'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Custom Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df = pd.read_excel(dict_file)\n",
    "dict_df = cbyz.df_lower(df=dict_df, cols=[])\n",
    "dict_li = dict_df['word'].tolist()\n",
    "custom_dict = dict((el, 1) for el in dict_li)\n",
    "custom_dict = construct_dictionary(custom_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. Load model\n",
    "# To use GPU:\n",
    "#    1. Install tensorflow-gpu (see Installation)\n",
    "#    2. Set CUDA_VISIBLE_DEVICES environment variable, e.g. os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "#    3. Set disable_cuda=False, e.g. ws = WS(\"./data\", disable_cuda=False)\n",
    "# To use CPU:\n",
    "# https://drive.google.com/drive/folders/105IKCb88evUyLKlLondvDBoh7Dy_I1tm\n",
    "\n",
    "ws = WS(path_resource + \"/data\")\n",
    "pos = POS(path_resource + \"/data\")\n",
    "ner = NER(path_resource + \"/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Bug - string要考慮重疊的問題')\n",
    "\n",
    "def filter_type(df, col, string, value):\n",
    "    '''\n",
    "    Keep rows with one type only\n",
    "    '''\n",
    "    assert isinstance(string, list), 'Error'\n",
    "    loc_df = df.copy()\n",
    "    type_li = [col + '_' + str(i) for i in range(len(string))]\n",
    "    \n",
    "    for i in range(len(string)):\n",
    "        loc_df[type_li[i]] = np.where(loc_df['title'].str.contains(string[i]), 1, 0)\n",
    "\n",
    "    loc_df = loc_df.melt(id_vars='index', var_name=col, value_vars=type_li)\n",
    "    loc_df = loc_df[loc_df['value']==1]\n",
    "    loc_df = cbyz.df_add_size(df=loc_df, group_by=['index'], col_name='count')\n",
    "    loc_df = loc_df[loc_df['count']==1]\n",
    "    loc_df = loc_df[['index', col]]\n",
    "    \n",
    "    cond = []\n",
    "    for i in range(len(type_li)):\n",
    "        cond.append(loc_df[col]==type_li[i])\n",
    "        \n",
    "    loc_df[col] = np.select(cond, value)\n",
    "    result = df.merge(loc_df, how='left', on='index')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s['word'].values.tolist(), \n",
    "                                                           s['pos'].values.tolist(), \n",
    "                                                           s['tag'].values.tolist())]\n",
    "        self.grouped = self.data.groupby('sentence').apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "    def get_next(self):\n",
    "        try: \n",
    "            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s \n",
    "        except:\n",
    "            return None\n",
    "                \n",
    "        \n",
    "def word2df(word_li, pos, y_li=[]):\n",
    "    result = pd.DataFrame()\n",
    "    pred_round = False\n",
    "    \n",
    "    if len(y_li) == 0:\n",
    "        y_li = [np.nan for i in range(len(word_li))]\n",
    "        pred_round = True\n",
    "    \n",
    "    for i in range(len(y_li)):\n",
    "        cur_sentence = word_li[i]\n",
    "        cur_y = y_li[i]\n",
    "\n",
    "        # cur_y不是na\n",
    "        cur_tag = []\n",
    "        for j in cur_sentence:\n",
    "            \n",
    "            if pred_round:\n",
    "                cur_tag.append(np.nan)\n",
    "            elif y == 'type':\n",
    "                if j == cur_y:\n",
    "                    cur_tag.append(1)\n",
    "                else:\n",
    "                    cur_tag.append(0)\n",
    "            else:\n",
    "                if j in cur_y:\n",
    "                    cur_tag.append(1)\n",
    "                else:\n",
    "                    cur_tag.append(0)\n",
    "\n",
    "        new_li = list(zip(cur_sentence,  pos[i], cur_tag))\n",
    "        new_result = pd.DataFrame(new_li, columns=['word', 'pos', 'tag'])\n",
    "        new_result['sentence'] = i\n",
    "\n",
    "        result = result.append(new_result)\n",
    "\n",
    "    return result        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0, \n",
    "        'word.lower()': word.lower(), \n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "        \n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run the WS-POS-NER pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_raw = pd.read_excel(data_file)\n",
    "\n",
    "# Use to merge original data\n",
    "data_raw['title_orig'] = data_raw['title']\n",
    "\n",
    "if 'index' not in data_raw.columns:\n",
    "    data_raw['index'] = data_raw.index\n",
    "\n",
    "data = data_raw.copy()\n",
    "\n",
    "# Preprocessing\n",
    "data = data.dropna(subset=['title'], axis=0)\n",
    "data = data[~data['title'].str.contains('廣告')]\n",
    "data = cbyz.df_lower(df=data, cols=['title', 'brand'])\n",
    "\n",
    "print('Bug here')\n",
    "data['title'] = data['title'].apply(cbyz.unicode_filter)\n",
    "# data['title'] = data['title'].apply(cbyz.unicode_filter, args=('(\\u0030-\\u007A|\\u4E00-\\u9FFF)'))\n",
    "\n",
    "# There are bug in the crawler, so clean title by limiting length of string.\n",
    "data = data[(data['title'].str.len()>=5) & (data['title'].str.len()<100)]\n",
    "\n",
    "# data = data[~data['name'].isna()]\n",
    "# data = data[data.index<=300]\n",
    "\n",
    "data = data.dropna(subset=['title'], axis=0)\n",
    "data['title'] = data['title'].str.replace('找相似', '')\n",
    "\n",
    "data = filter_type(data, \n",
    "                   string=['淡香水', '香水', '淡香精', '香精', '古龍水'],\n",
    "                   value=['淡香水', '香水', '淡香精', '香精', '古龍水'],\n",
    "                   col='type')\n",
    "\n",
    "data = filter_type(data, \n",
    "                   string=['男', '女', '中性'],\n",
    "                   value=['男性', '女性', '中性'],\n",
    "                   col='gender')\n",
    "\n",
    "print('Bug - title裡面還是有括號')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 'name'\n",
    "# y = 'brand'\n",
    "# y = 'type'\n",
    "y_pred_col = y + '_pred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = data[~data[y].isna()]\n",
    "title_train = train_raw['title'].tolist()\n",
    "\n",
    "pred_raw = data[data[y].isna()]\n",
    "title_pred = pred_raw['title'].tolist()\n",
    "\n",
    "y_train = train_raw[y].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sentence_train = ws(\n",
    "    title_train,\n",
    "    # sentence_segmentation = True, # To consider delimiters\n",
    "    # segment_delimiter_set = {\",\", \"。\", \":\", \"?\", \"!\", \";\"}), # This is the defualt set of delimiters\n",
    "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "    coerce_dictionary = custom_dict, # words in this dictionary are forced\n",
    ")\n",
    "\n",
    "pos_sentence_train = pos(word_sentence_train)\n",
    "entity_sentence_train = ner(word_sentence_train, pos_sentence_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sentence_pred = ws(\n",
    "    title_pred,\n",
    "    # sentence_segmentation = True, # To consider delimiters\n",
    "    # segment_delimiter_set = {\",\", \"。\", \":\", \"?\", \"!\", \";\"}), # This is the defualt set of delimiters\n",
    "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "    coerce_dictionary = custom_dict, # words in this dictionary are forced\n",
    ")\n",
    "\n",
    "pos_sentence_pred = pos(word_sentence_pred)\n",
    "entity_sentence_pred = ner(word_sentence_pred, pos_sentence_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = word2df(word_li=word_sentence_train, pos=pos_sentence_train, y_li=y_train)\n",
    "df_train = df_train[['sentence', 'word', 'pos', 'tag']]\n",
    "df_train = cbyz.df_conv_col_type(df=df_train, cols='tag', to='str')\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = word2df(word_li=word_sentence_pred, pos=pos_sentence_pred)\n",
    "df_pred = df_pred[['sentence', 'word', 'pos', 'tag']]\n",
    "df_pred = cbyz.df_conv_col_type(df=df_pred, cols='tag', to='str')\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Show Results\n",
    "def print_word_pos_sentence(word_sentence, pos_sentence):\n",
    "    assert len(word_sentence) == len(pos_sentence)\n",
    "    for word, pos in zip(word_sentence, pos_sentence):\n",
    "        print(f\"{word}({pos})\", end=\"\\u3000\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, sentence in enumerate(sentence_list):\n",
    "#     print(f\"'{sentence}'\")\n",
    "#     print_word_pos_sentence(word_sentence_list[i],  pos_sentence_list[i])\n",
    "#     for entity in sorted(entity_sentence_list[i]):\n",
    "#         print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter_train = SentenceGetter(df_train)\n",
    "sentence_train = getter_train.sentences\n",
    "\n",
    "getter_pred = SentenceGetter(df_pred)\n",
    "sentence_pred = getter_pred.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split train and test sets\n",
    "X = [sent2features(s) for s in sentence_train]\n",
    "new_y = [sent2labels(s) for s in sentence_train]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, new_y, test_size=0.2, random_state=0)\n",
    "# X_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = [sent2features(s) for s in sentence_pred]\n",
    "# new_y = [sent2labels(s) for s in sentence_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycrfsuite\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "#     'c1': 1.0,   # coefficient for L1 penalty\n",
    "#     'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n",
    "# Trainimg\n",
    "trainer.train(path_export + '/ckip_tagger_model.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.logparser.last_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trainer.logparser.iterations), trainer.logparser.iterations[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "\n",
    "To use the trained model, create pycrfsuite.Tagger, open the model and use \"tag\" method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open(path_export + '/ckip_tagger_model.crfsuite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    new_pred = tagger.tag(X_test[i])\n",
    "    y_test_pred.append(new_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = cbyz.li_flatten(y_test_pred)\n",
    "y_test = cbyz.li_flatten(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_pred=y_test_pred, y_true=y_test))\n",
    "# print(classification_report(y_pred=y_pred, y_true=y_test, labels=new_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for i in range(len(X_pred)):\n",
    "    new_pred = tagger.tag(X_pred[i])\n",
    "    y_pred = y_pred + new_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = df_pred.copy()\n",
    "pred_result['tag'] = y_pred\n",
    "pred_result = pred_result[pred_result['tag']=='1']\n",
    "pred_result = pred_result[['sentence', 'word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pred[~df_pred['tag'].isna()]\n",
    "if y == 'name':\n",
    "    pred_result = pred_result \\\n",
    "                    .pivot_table(index=['sentence'],\n",
    "                                 values='word',\n",
    "                                 aggfunc=lambda x: ''.join(x)) \\\n",
    "                    .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_raw['title_orig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "update_file = pred_raw[['title_orig']]\n",
    "update_file['sentence'] = pred_raw.index\n",
    "print(len(update_file))\n",
    "\n",
    "assert len(update_file) >= len(pred_result), 'Error'\n",
    "update_file = update_file.merge(pred_result, how='left', on='sentence')\n",
    "\n",
    "update_file = update_file[['title_orig', 'word']]\n",
    "update_file.columns = ['title', y_pred_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_file[~update_file['name_pred'].isna()]\n",
    "# update_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_file[~update_file['name_pred'].isna()]\n",
    "pred_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Console\n",
    "update_file[update_file.index<=20]\n",
    "# pred_result[pred_result['sentence']==13]\n",
    "# pred_result\n",
    "# update_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_file[~update_file['name_pred'].isna()]\n",
    "update_file[update_file.index==13]['title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if y_pred_col in data_raw.columns:\n",
    "    data_raw = data_raw.drop(y_pred_col, axis=1)\n",
    "\n",
    "update_file = data_raw.merge(update_file, how='left', on='title')\n",
    "update_file = update_file.drop('title_orig', axis=1)\n",
    "\n",
    "update_file = update_file.reset_index(drop=True)\n",
    "update_file['index'] = update_file.index\n",
    "\n",
    "update_file.to_excel(data_file, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check what classifier learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "info = tagger.info()\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common(15))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common()[-15:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, for example, it is very likely that the beginning of an organization name (B-ORG) will be followed by a token inside organization name (I-ORG), but transitions to I-ORG from tokens with other labels are penalized. Also note I-PER -> B-LOC transition: a positive weight means that model thinks that a person name is often followed by a location.\n",
    "\n",
    "Check the state features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-6s %s\" % (weight, label, attr))    \n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(info.state_features).most_common(20))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(info.state_features).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import matutils\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        path_resource + '/tmunlp_1.6B_WB_50dim_2020v1.bin.gz', \n",
    "        unicode_errors='ignore',\n",
    "        binary=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "\n",
    "# wv_data = word_sentence_list\n",
    "wv_data = word_sentence_list[0]\n",
    "\n",
    "result = pd.DataFrame()\n",
    "\n",
    "for i in range(len(wv_data)):\n",
    "\n",
    "    word_vec = {}\n",
    "    for w in word_sentence_list[i]:\n",
    "        try:\n",
    "            word_vec[w] = model.get_vector(w)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Create DF\n",
    "    keys = list(word_vec.keys())\n",
    "    vec_df = pd.DataFrame({'word':keys,\n",
    "                          'index':range(len(keys))})\n",
    "\n",
    "    vec_df = cbyz.df_cross_join(vec_df, vec_df)\n",
    "    vec_df = vec_df[vec_df['index_x']<vec_df['index_y']] \\\n",
    "            .reset_index(drop=True) \n",
    "    \n",
    "    for j in range(len(vec_df)):\n",
    "        vec1 = model.get_vector(vec_df.loc[j, 'word_x'])\n",
    "        vec2 = model.get_vector(vec_df.loc[j, 'word_y'])\n",
    "        similarity = np.dot(matutils.unitvec(vec1), matutils.unitvec(vec2))\n",
    "        vec_df.loc[j, 'similarity'] = similarity\n",
    "\n",
    "    result = result.append(vec_df)\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print(i, '/', len(word_sentence_list))\n",
    "    \n",
    "result = result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "\n",
    "wv_data = word_sentence_list\n",
    "# wv_data = word_sentence_list[0]\n",
    "\n",
    "result = pd.DataFrame()\n",
    "\n",
    "for i in range(len(wv_data)):\n",
    "\n",
    "    word_vec = {}\n",
    "    for w in word_sentence_list[i]:\n",
    "        try:\n",
    "            word_vec[w] = model.get_vector(w)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Create DF\n",
    "    keys = list(word_vec.keys())\n",
    "    vec_df = pd.DataFrame({'word':keys,\n",
    "                          'index':range(len(keys))})\n",
    "\n",
    "    for j in range(1, len(vec_df)):\n",
    "        vec1 = model.get_vector(vec_df.loc[j, 'word'])\n",
    "        vec2 = model.get_vector(vec_df.loc[j-1, 'word'])\n",
    "        similarity = np.dot(matutils.unitvec(vec1), matutils.unitvec(vec2))\n",
    "        vec_df.loc[j, 'similarity'] = similarity\n",
    "\n",
    "    result = result.append(vec_df)\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print(i, '/', len(word_sentence_list))\n",
    "    \n",
    "result = result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sentence_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_excel(path_export + '/wv_result.xlsx', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "寶格麗\n",
    "馨香\n",
    "浪漫\n",
    "玫香\n",
    "淡香精\n",
    "\n",
    "bvlgari\n",
    "寶格麗\n",
    "晶澈\n",
    "女性\n",
    "淡香水\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[result['similarity']>0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.vectors\n",
    "# model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - 跟淡香水之間的距離\n",
    "#   和男性、女性、中性的距離\n",
    "\n",
    "# v0.0302\n",
    "# - 如果title中同時出現「香水」、「香精」、「古龍」兩個以上就排除"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
