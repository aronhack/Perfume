{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER\n",
    "from ckiptagger import construct_dictionary\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "host = 4\n",
    "# host = 0\n",
    "\n",
    "if host == 0:\n",
    "    path = r'/Users/aron/Documents/GitHub/Perfume/2_NLP'\n",
    "    dict_file = r'/Users/aron/Documents/GitHub/Perfume/1_Crawler/Resource/dict.xlsx'\n",
    "    data_file = r'/Users/aron/Documents/GitHub/Perfume/1_Crawler/Resource/data.xls'\n",
    "    \n",
    "elif host == 4:\n",
    "    path = r'/home/rserver/Data_Mining/personal_workspace/yz/Lab/CkipTagger/'\n",
    "    dict_file = r'/home/rserver/Data_Mining/personal_workspace/yz/Lab/CkipTagger/Resource/dict.xlsx'\n",
    "    data_file = r'/home/rserver/Data_Mining/personal_workspace/yz/Lab/CkipTagger/Resource/data.xls'\n",
    "\n",
    "    \n",
    "# Codebase ......\n",
    "path_codebase = [r'/Users/aron/Documents/GitHub/Arsenal/',\n",
    "                 r'/home/aronhack/stock_predict/Function',\n",
    "                 r'D:\\GitHub\\Arsenal',\n",
    "                 r'D:\\Data_Mining\\Projects\\Codebase_YZ',\n",
    "                 r'/home/jupyter/Arsenal/20220522',\n",
    "                 path + '/Function']\n",
    "\n",
    "\n",
    "for i in path_codebase:    \n",
    "    if i not in sys.path:\n",
    "        sys.path = [i] + sys.path\n",
    "    \n",
    "\n",
    "import codebase_yz as cbyz\n",
    "    \n",
    "    \n",
    "path_resource = path + '/Resource'\n",
    "path_export = path + '/Export'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Custom Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df = pd.read_excel(dict_file)    \n",
    "dict_li = dict_df['word'].tolist()\n",
    "custom_dict = dict((el, 1) for el in dict_li)\n",
    "custom_dict = construct_dictionary(custom_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ckiptagger/model_ws.py:106: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:988: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  partitioner=maybe_partitioner)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:996: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  initializer=initializer)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ckiptagger/model_pos.py:56: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ckiptagger/model_ner.py:57: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n"
     ]
    }
   ],
   "source": [
    "# 2. Load model\n",
    "# To use GPU:\n",
    "#    1. Install tensorflow-gpu (see Installation)\n",
    "#    2. Set CUDA_VISIBLE_DEVICES environment variable, e.g. os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "#    3. Set disable_cuda=False, e.g. ws = WS(\"./data\", disable_cuda=False)\n",
    "# To use CPU:\n",
    "# https://drive.google.com/drive/folders/105IKCb88evUyLKlLondvDBoh7Dy_I1tm\n",
    "\n",
    "ws = WS(path_resource + \"/data\")\n",
    "pos = POS(path_resource + \"/data\")\n",
    "ner = NER(path_resource + \"/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run the WS-POS-NER pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(data_file)\n",
    "data = data.dropna(subset=['title'], axis=0)\n",
    "data = data[~data['title'].str.contains('廣告')]\n",
    "\n",
    "# data = data[~data['name'].isna()]\n",
    "# data = data[data.index<=300]\n",
    "\n",
    "data = data.dropna(subset=['title'], axis=0)\n",
    "sentence_list = data['title'].tolist()\n",
    "\n",
    "# y = data['name'].tolist()\n",
    "y = data['brand'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Manually\n",
    "data = data[data['title'].str.contains(' 萬寶龍星際旅者男性淡香水')]\n",
    "sentence_list = data['title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sentence_list = ws(\n",
    "    sentence_list,\n",
    "    # sentence_segmentation = True, # To consider delimiters\n",
    "    # segment_delimiter_set = {\",\", \"。\", \":\", \"?\", \"!\", \";\"}), # This is the defualt set of delimiters\n",
    "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "    coerce_dictionary = custom_dict, # words in this dictionary are forced\n",
    ")\n",
    "\n",
    "pos_sentence_list = pos(word_sentence_list)\n",
    "entity_sentence_list = ner(word_sentence_list, pos_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['HUAHUA',\n",
       "  '香水',\n",
       "  '美妝',\n",
       "  ' Montblanc ',\n",
       "  'Starwalker ',\n",
       "  '萬寶龍',\n",
       "  '星際',\n",
       "  '旅者',\n",
       "  '男性',\n",
       "  '淡香水',\n",
       "  ' ',\n",
       "  '50',\n",
       "  ' ',\n",
       "  '75',\n",
       "  'ML ',\n",
       "  '【',\n",
       "  '全新',\n",
       "  '正品',\n",
       "  '】',\n",
       "  '$',\n",
       "  '880',\n",
       "  ' - $',\n",
       "  '930',\n",
       "  '已',\n",
       "  '售出',\n",
       "  ' 433',\n",
       "  '臺北市',\n",
       "  '中山區',\n",
       "  '找',\n",
       "  '相似'],\n",
       " ['MONTBLANC',\n",
       "  ' ',\n",
       "  '萬寶龍',\n",
       "  '星際',\n",
       "  '旅',\n",
       "  '者',\n",
       "  '男性',\n",
       "  '淡香水',\n",
       "  '75',\n",
       "  'ml',\n",
       "  '滿額',\n",
       "  '折$',\n",
       "  '30',\n",
       "  '$',\n",
       "  '850',\n",
       "  '已',\n",
       "  '售出',\n",
       "  ' 8',\n",
       "  '嘉義市',\n",
       "  '東區',\n",
       "  '找',\n",
       "  '相似'],\n",
       " ['蝦皮',\n",
       "  '優選',\n",
       "  'Montblanc ',\n",
       "  'Starwalker ',\n",
       "  '萬寶龍',\n",
       "  '星際',\n",
       "  '旅者',\n",
       "  '男性',\n",
       "  '淡香水',\n",
       "  ' ',\n",
       "  '75',\n",
       "  'ml ',\n",
       "  '50',\n",
       "  'ml ',\n",
       "  '另',\n",
       "  '有',\n",
       "  'TESTER',\n",
       "  '【',\n",
       "  '金多利',\n",
       "  '美妝',\n",
       "  '】',\n",
       "  '$',\n",
       "  '880',\n",
       "  ' - $',\n",
       "  '990',\n",
       "  '已',\n",
       "  '售出',\n",
       "  ' 28',\n",
       "  '新北市',\n",
       "  '板橋區',\n",
       "  '找',\n",
       "  '相似'],\n",
       " ['”',\n",
       "  '代理商',\n",
       "  '公司貨',\n",
       "  '”',\n",
       "  'Montblanc ',\n",
       "  'Starwalker ',\n",
       "  '萬寶龍',\n",
       "  '星際',\n",
       "  '旅',\n",
       "  '者',\n",
       "  '男性',\n",
       "  '淡香水',\n",
       "  '50',\n",
       "  'ML/75',\n",
       "  'ML',\n",
       "  '$',\n",
       "  '880',\n",
       "  ' - ',\n",
       "  '$',\n",
       "  '990',\n",
       "  '嘉義縣',\n",
       "  '大林鎮',\n",
       "  '找',\n",
       "  '相似'],\n",
       " ['分享',\n",
       "  '香 ',\n",
       "  'Montblanc ',\n",
       "  'Starwalker ',\n",
       "  '萬寶龍',\n",
       "  '星際',\n",
       "  '旅',\n",
       "  '者',\n",
       "  '男性',\n",
       "  '淡香水',\n",
       "  '分',\n",
       "  '裝香',\n",
       "  '5',\n",
       "  'ml.',\n",
       "  '10',\n",
       "  'ml',\n",
       "  '$',\n",
       "  '65',\n",
       "  '已',\n",
       "  '售出',\n",
       "  ' 6',\n",
       "  '雲林縣',\n",
       "  '西螺鎮',\n",
       "  '找',\n",
       "  '相似']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for i, sentence in enumerate(sentence_list):\n",
    "    \n",
    "    new_li = list(zip(word_sentence_list[i],  pos_sentence_list[i]))\n",
    "    cur_y = y[i]\n",
    "    \n",
    "    temp_df = pd.DataFrame(new_li, columns=['word', 'pos'])\n",
    "    temp_df['sentence'] = i\n",
    "    temp_df['tag'] = np.where(temp_df['word']==cur_y, '1', '0')\n",
    "    \n",
    "    df = df.append(temp_df)\n",
    "    \n",
    "df = df[['sentence', 'word', 'pos', 'tag']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['tag']=='1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Show Results\n",
    "def print_word_pos_sentence(word_sentence, pos_sentence):\n",
    "    assert len(word_sentence) == len(pos_sentence)\n",
    "    for word, pos in zip(word_sentence, pos_sentence):\n",
    "        print(f\"{word}({pos})\", end=\"\\u3000\")\n",
    "    print()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_sentence_list[0]\n",
    "# pos_sentence_list[0]\n",
    "\n",
    "df[df['tag']=='1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(sentence_list):\n",
    "#     print()\n",
    "    print(f\"'{sentence}'\")\n",
    "    print_word_pos_sentence(word_sentence_list[i],  pos_sentence_list[i])\n",
    "    for entity in sorted(entity_sentence_list[i]):\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s['word'].values.tolist(), \n",
    "                                                           s['pos'].values.tolist(), \n",
    "                                                           s['tag'].values.tolist())]\n",
    "        self.grouped = self.data.groupby('sentence').apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "    def get_next(self):\n",
    "        try: \n",
    "            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s \n",
    "        except:\n",
    "            return None\n",
    "\n",
    "        \n",
    "getter = SentenceGetter(df)\n",
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0, \n",
    "        'word.lower()': word.lower(), \n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "        \n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split train and test sets\n",
    "X = [sent2features(s) for s in sentences]\n",
    "new_y = [sent2labels(s) for s in sentences]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, new_y, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycrfsuite\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "#     'c1': 1.0,   # coefficient for L1 penalty\n",
    "#     'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(path_export + '/ckip_tagger_model.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.logparser.last_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trainer.logparser.iterations), trainer.logparser.iterations[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "\n",
    "To use the trained model, create pycrfsuite.Tagger, open the model and use \"tag\" method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open(path_export + '/ckip_tagger_model.crfsuite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tag a sentence to see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for i in range(len(y_test)):\n",
    "    new_pred = tagger.tag(X_test[i])\n",
    "    y_pred.append(new_pred)\n",
    "#     example_sent = X_test[i]\n",
    "#     print(\"Predicted:\", ' '.join(tagger.tag(X_test[i])))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cbyz.li_flatten(y_pred)\n",
    "y_test = cbyz.li_flatten(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(y_pred)\n",
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[i for i in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_pred=y_pred, y_true=y_test))\n",
    "# print(classification_report(y_pred=y_pred, y_true=y_test, labels=new_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check what classifier learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "info = tagger.info()\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common(15))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common()[-15:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, for example, it is very likely that the beginning of an organization name (B-ORG) will be followed by a token inside organization name (I-ORG), but transitions to I-ORG from tokens with other labels are penalized. Also note I-PER -> B-LOC transition: a positive weight means that model thinks that a person name is often followed by a location.\n",
    "\n",
    "Check the state features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-6s %s\" % (weight, label, attr))    \n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(info.state_features).most_common(20))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(info.state_features).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Worklist\n",
    "# - 跟淡香水之間的距離\n",
    "#   和男性、女性、中性的距離\n",
    "# - 如果title中同時出現「淡香水」、「淡香精」就排除"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
