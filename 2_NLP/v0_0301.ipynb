{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER\n",
    "from ckiptagger import construct_dictionary\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "host = 4\n",
    "# host = 0\n",
    "\n",
    "if host == 0:\n",
    "    path = r'/Users/aron/Documents/GitHub/Perfume/2_NLP'\n",
    "    dict_file = r'/Users/aron/Documents/GitHub/Perfume/1_Crawler/Resource/dict.xlsx'\n",
    "    data_file = r'/Users/aron/Documents/GitHub/Perfume/1_Crawler/Resource/data.xls'\n",
    "    \n",
    "elif host == 4:\n",
    "    path = r'/home/rserver/Data_Mining/personal_workspace/yz/Lab/CkipTagger/'\n",
    "    dict_file = r'/home/rserver/Data_Mining/personal_workspace/yz/Lab/CkipTagger/Resource/dict.xlsx'\n",
    "    data_file = r'/home/rserver/Data_Mining/personal_workspace/yz/Lab/CkipTagger/Resource/data.xls'\n",
    "\n",
    "    \n",
    "# Codebase ......\n",
    "path_codebase = [r'/Users/aron/Documents/GitHub/Arsenal/',\n",
    "                 r'/Users/aron/Documents/GitHub/Codebase_YZ/',\n",
    "                 r'/home/aronhack/stock_predict/Function',\n",
    "                 r'D:\\GitHub\\Arsenal',\n",
    "                 r'D:\\Data_Mining\\Projects\\Codebase_YZ',\n",
    "                 r'/home/jupyter/Arsenal/20220522',\n",
    "                 path + '/Function']\n",
    "\n",
    "\n",
    "for i in path_codebase:    \n",
    "    if i not in sys.path:\n",
    "        sys.path = [i] + sys.path\n",
    "    \n",
    "import codebase_yz as cbyz\n",
    "    \n",
    "path_resource = path + '/Resource'\n",
    "path_export = path + '/Export'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Custom Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df = pd.read_excel(dict_file)\n",
    "dict_df = cbyz.df_lower(df=dict_df, cols=[])\n",
    "dict_li = dict_df['word'].tolist()\n",
    "custom_dict = dict((el, 1) for el in dict_li)\n",
    "custom_dict = construct_dictionary(custom_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ckiptagger/model_ws.py:106: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:988: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  partitioner=maybe_partitioner)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:996: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  initializer=initializer)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ckiptagger/model_pos.py:56: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ckiptagger/model_ner.py:57: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n"
     ]
    }
   ],
   "source": [
    "# 2. Load model\n",
    "# To use GPU:\n",
    "#    1. Install tensorflow-gpu (see Installation)\n",
    "#    2. Set CUDA_VISIBLE_DEVICES environment variable, e.g. os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "#    3. Set disable_cuda=False, e.g. ws = WS(\"./data\", disable_cuda=False)\n",
    "# To use CPU:\n",
    "# https://drive.google.com/drive/folders/105IKCb88evUyLKlLondvDBoh7Dy_I1tm\n",
    "\n",
    "ws = WS(path_resource + \"/data\")\n",
    "pos = POS(path_resource + \"/data\")\n",
    "ner = NER(path_resource + \"/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run the WS-POS-NER pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bug - title裡面還是有括號\n"
     ]
    }
   ],
   "source": [
    "data_raw = pd.read_excel(data_file)\n",
    "\n",
    "# Use to merge original data\n",
    "data_raw['title_orig'] = data_raw['title']\n",
    "\n",
    "data = data_raw.copy()\n",
    "\n",
    "# Preprocessing\n",
    "data = data.dropna(subset=['title'], axis=0)\n",
    "data = data[~data['title'].str.contains('廣告')]\n",
    "data = cbyz.df_lower(df=data, cols=['title', 'brand'])\n",
    "data['title'] = data['title'].apply(cbyz.unicode_filter)\n",
    "\n",
    "# There are bug in the crawler, so clean title by limiting length of string.\n",
    "data = data[(data['title'].str.len()>=5) & (data['title'].str.len()<100)]\n",
    "\n",
    "# data = data[~data['name'].isna()]\n",
    "# data = data[data.index<=300]\n",
    "\n",
    "data = data.dropna(subset=['title'], axis=0)\n",
    "data['title'] = data['title'].str.replace('找相似', '')\n",
    "\n",
    "print('Bug - title裡面還是有括號')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 'name'\n",
    "y = 'brand'\n",
    "y = 'type'\n",
    "y_pred_col = y + '_pred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = data[~data[y].isna()]\n",
    "title_train = train_raw['title'].tolist()\n",
    "\n",
    "pred_raw = data[data[y].isna()]\n",
    "title_pred = pred_raw['title'].tolist()\n",
    "\n",
    "y_train = train_raw[y].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sentence_train = ws(\n",
    "    title_train,\n",
    "    # sentence_segmentation = True, # To consider delimiters\n",
    "    # segment_delimiter_set = {\",\", \"。\", \":\", \"?\", \"!\", \";\"}), # This is the defualt set of delimiters\n",
    "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "    coerce_dictionary = custom_dict, # words in this dictionary are forced\n",
    ")\n",
    "\n",
    "pos_sentence_train = pos(word_sentence_train)\n",
    "entity_sentence_train = ner(word_sentence_train, pos_sentence_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sentence_pred = ws(\n",
    "    title_pred,\n",
    "    # sentence_segmentation = True, # To consider delimiters\n",
    "    # segment_delimiter_set = {\",\", \"。\", \":\", \"?\", \"!\", \";\"}), # This is the defualt set of delimiters\n",
    "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "    coerce_dictionary = custom_dict, # words in this dictionary are forced\n",
    ")\n",
    "\n",
    "pos_sentence_pred = pos(word_sentence_pred)\n",
    "entity_sentence_pred = ner(word_sentence_pred, pos_sentence_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame()\n",
    "# for i, sentence in enumerate(y_train):\n",
    "    \n",
    "#     cur_sentence = word_sentence_train[i]\n",
    "#     cur_y = y_train[i]\n",
    "    \n",
    "\n",
    "#     # cur_y不是na\n",
    "#     cur_tag = []\n",
    "#     for j in cur_sentence:\n",
    "\n",
    "#         if y == 'type':\n",
    "#             if j == cur_y:\n",
    "#                 cur_tag.append(1)\n",
    "#             else:\n",
    "#                 cur_tag.append(0)\n",
    "#         else:\n",
    "#             if j in cur_y:\n",
    "#                 cur_tag.append(1)\n",
    "#             else:\n",
    "#                 cur_tag.append(0)\n",
    "                \n",
    "#     new_li = list(zip(cur_sentence,  pos_sentence_list[i], cur_tag))\n",
    "#     temp_df = pd.DataFrame(new_li, columns=['word', 'pos', 'tag'])\n",
    "#     temp_df['sentence'] = i\n",
    "    \n",
    "#     df = df.append(temp_df)\n",
    "    \n",
    "# df = df[['sentence', 'word', 'pos', 'tag']]\n",
    "# df = cbyz.df_conv_col_type(df=df, cols='tag', to='str')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2df(word_li, pos, y_li=[]):\n",
    "    \n",
    "    result = pd.DataFrame()\n",
    "    pred_round = False\n",
    "    \n",
    "    if len(y_li) == 0:\n",
    "        y_li = [np.nan for i in range(len(word_li))]\n",
    "        pred_round = True\n",
    "    \n",
    "    \n",
    "    for i in range(len(y_li)):\n",
    "        \n",
    "        cur_sentence = word_li[i]\n",
    "        cur_y = y_li[i]\n",
    "\n",
    "        # cur_y不是na\n",
    "        cur_tag = []\n",
    "        for j in cur_sentence:\n",
    "            \n",
    "            if pred_round:\n",
    "                cur_tag.append(np.nan)\n",
    "            elif y == 'type':\n",
    "                if j == cur_y:\n",
    "                    cur_tag.append(1)\n",
    "                else:\n",
    "                    cur_tag.append(0)\n",
    "            else:\n",
    "                if j in cur_y:\n",
    "                    cur_tag.append(1)\n",
    "                else:\n",
    "                    cur_tag.append(0)\n",
    "\n",
    "        new_li = list(zip(cur_sentence,  pos[i], cur_tag))\n",
    "        new_result = pd.DataFrame(new_li, columns=['word', 'pos', 'tag'])\n",
    "        new_result['sentence'] = i\n",
    "\n",
    "        result = result.append(new_result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>Neu</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>折</td>\n",
       "      <td>Nf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>現貨</td>\n",
       "      <td>Na</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>clausportomusgorealblackedition</td>\n",
       "      <td>FW</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>Neu</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>35</td>\n",
       "      <td>已</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>35</td>\n",
       "      <td>售出</td>\n",
       "      <td>VC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>35</td>\n",
       "      <td>116</td>\n",
       "      <td>Neu</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>35</td>\n",
       "      <td>桃園市</td>\n",
       "      <td>Nc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>35</td>\n",
       "      <td>龜山區</td>\n",
       "      <td>Nc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>763 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentence                             word  pos tag\n",
       "0          0                               95  Neu   0\n",
       "1          0                                折   Nf   0\n",
       "2          0                               現貨   Na   0\n",
       "3          0  clausportomusgorealblackedition   FW   0\n",
       "4          0                              100  Neu   0\n",
       "..       ...                              ...  ...  ..\n",
       "28        35                                已    D   0\n",
       "29        35                               售出   VC   0\n",
       "30        35                              116  Neu   0\n",
       "31        35                              桃園市   Nc   0\n",
       "32        35                              龜山區   Nc   0\n",
       "\n",
       "[763 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = word2df(word_li=word_sentence_train, pos=pos_sentence_train, y_li=y_train)\n",
    "df_train = df_train[['sentence', 'word', 'pos', 'tag']]\n",
    "df_train = cbyz.df_conv_col_type(df=df_train, cols='tag', to='str')\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>現貨</td>\n",
       "      <td>Na</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>速出</td>\n",
       "      <td>VCL</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>皇室</td>\n",
       "      <td>Nc</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>品牌</td>\n",
       "      <td>Na</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>clausporto</td>\n",
       "      <td>FW</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2451</td>\n",
       "      <td>已</td>\n",
       "      <td>D</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2451</td>\n",
       "      <td>售出</td>\n",
       "      <td>VC</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2451</td>\n",
       "      <td>1</td>\n",
       "      <td>Neu</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2451</td>\n",
       "      <td>臺中市</td>\n",
       "      <td>Nc</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2451</td>\n",
       "      <td>南屯區</td>\n",
       "      <td>Nc</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41386 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentence        word  pos  tag\n",
       "0          0          現貨   Na  nan\n",
       "1          0          速出  VCL  nan\n",
       "2          0          皇室   Nc  nan\n",
       "3          0          品牌   Na  nan\n",
       "4          0  clausporto   FW  nan\n",
       "..       ...         ...  ...  ...\n",
       "9       2451           已    D  nan\n",
       "10      2451          售出   VC  nan\n",
       "11      2451           1  Neu  nan\n",
       "12      2451         臺中市   Nc  nan\n",
       "13      2451         南屯區   Nc  nan\n",
       "\n",
       "[41386 rows x 4 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred = word2df(word_li=word_sentence_pred, pos=pos_sentence_pred)\n",
    "df_pred = df_pred[['sentence', 'word', 'pos', 'tag']]\n",
    "df_pred = cbyz.df_conv_col_type(df=df_pred, cols='tag', to='str')\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 36)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_sentence_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Show Results\n",
    "def print_word_pos_sentence(word_sentence, pos_sentence):\n",
    "    assert len(word_sentence) == len(pos_sentence)\n",
    "    for word, pos in zip(word_sentence, pos_sentence):\n",
    "        print(f\"{word}({pos})\", end=\"\\u3000\")\n",
    "    print()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, sentence in enumerate(sentence_list):\n",
    "#     print(f\"'{sentence}'\")\n",
    "#     print_word_pos_sentence(word_sentence_list[i],  pos_sentence_list[i])\n",
    "#     for entity in sorted(entity_sentence_list[i]):\n",
    "#         print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worklist\n",
    "# 1. Add types of perfume in the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s['word'].values.tolist(), \n",
    "                                                           s['pos'].values.tolist(), \n",
    "                                                           s['tag'].values.tolist())]\n",
    "        self.grouped = self.data.groupby('sentence').apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "    def get_next(self):\n",
    "        try: \n",
    "            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s \n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "getter_train = SentenceGetter(df_train)\n",
    "sentence_train = getter_train.sentences\n",
    "\n",
    "getter_pred = SentenceGetter(df_pred)\n",
    "sentence_pred = getter_pred.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0, \n",
    "        'word.lower()': word.lower(), \n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "        \n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split train and test sets\n",
    "X = [sent2features(s) for s in sentence_train]\n",
    "new_y = [sent2labels(s) for s in sentence_train]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, new_y, test_size=0.2, random_state=0)\n",
    "\n",
    "# X_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = [sent2features(s) for s in sentence_pred]\n",
    "# new_y = [sent2labels(s) for s in sentence_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycrfsuite\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "#     'c1': 1.0,   # coefficient for L1 penalty\n",
    "#     'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n",
    "# Trainimg\n",
    "trainer.train(path_export + '/ckip_tagger_model.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature.minfreq',\n",
       " 'feature.possible_states',\n",
       " 'feature.possible_transitions',\n",
       " 'c1',\n",
       " 'c2',\n",
       " 'max_iterations',\n",
       " 'num_memories',\n",
       " 'epsilon',\n",
       " 'period',\n",
       " 'delta',\n",
       " 'linesearch',\n",
       " 'max_linesearch']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 27,\n",
       " 'scores': {},\n",
       " 'loss': 34.151758,\n",
       " 'feature_norm': 3.739187,\n",
       " 'error_norm': 0.00421,\n",
       " 'active_features': 1443,\n",
       " 'linesearch_trials': 1,\n",
       " 'linesearch_step': 1.0,\n",
       " 'time': 0.001}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.logparser.last_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 {'num': 27, 'scores': {}, 'loss': 34.151758, 'feature_norm': 3.739187, 'error_norm': 0.00421, 'active_features': 1443, 'linesearch_trials': 1, 'linesearch_step': 1.0, 'time': 0.001}\n"
     ]
    }
   ],
   "source": [
    "print(len(trainer.logparser.iterations), trainer.logparser.iterations[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "\n",
    "To use the trained model, create pycrfsuite.Tagger, open the model and use \"tag\" method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x7fc5a8cdf210>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open(path_export + '/ckip_tagger_model.crfsuite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    new_pred = tagger.tag(X_test[i])\n",
    "    y_test_pred.append(new_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = cbyz.li_flatten(y_test_pred)\n",
    "y_test = cbyz.li_flatten(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       174\n",
      "           1       1.00      0.75      0.86         8\n",
      "\n",
      "    accuracy                           0.99       182\n",
      "   macro avg       0.99      0.88      0.93       182\n",
      "weighted avg       0.99      0.99      0.99       182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred=y_test_pred, y_true=y_test))\n",
    "# print(classification_report(y_pred=y_pred, y_true=y_test, labels=new_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for i in range(len(X_pred)):\n",
    "    new_pred = tagger.tag(X_pred[i])\n",
    "    y_pred.append(new_pred)\n",
    "    \n",
    "y_pred_flatten = cbyz.li_flatten(y_pred)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = df_pred.copy()\n",
    "pred_result['tag'] = y_pred_flatten\n",
    "pred_result = pred_result[pred_result['tag']=='1']\n",
    "pred_result = pred_result[['sentence', 'word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "update_file = pred_raw[['title_orig']]\n",
    "update_file['sentence'] = pred_raw.index\n",
    "update_file = update_file.merge(pred_result, how='left', on='sentence')\n",
    "update_file = update_file[['title_orig', 'word']]\n",
    "update_file.columns = ['title', y_pred_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: As the xlwt package is no longer maintained, the xlwt engine will be removed in a future version of pandas. This is the only engine in pandas that supports writing in the xls format. Install openpyxl and write to an xlsx file instead. You can set the option io.excel.xls.writer to 'xlwt' to silence this warning. While this option is deprecated and will also raise a warning, it can be globally set and the warning suppressed.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "if y_pred_col in data_raw.columns:\n",
    "    data_raw = data_raw.drop(y_pred_col, axis=1)\n",
    "\n",
    "update_file = data_raw.merge(update_file, how='left', on='title')\n",
    "update_file = update_file.drop('title_orig', axis=1)\n",
    "update_file.to_excel(data_file, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check what classifier learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "info = tagger.info()\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common(15))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common()[-15:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, for example, it is very likely that the beginning of an organization name (B-ORG) will be followed by a token inside organization name (I-ORG), but transitions to I-ORG from tokens with other labels are penalized. Also note I-PER -> B-LOC transition: a positive weight means that model thinks that a person name is often followed by a location.\n",
    "\n",
    "Check the state features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-6s %s\" % (weight, label, attr))    \n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(info.state_features).most_common(20))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(info.state_features).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import matutils\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        path_resource + '/tmunlp_1.6B_WB_50dim_2020v1.bin.gz', \n",
    "        unicode_errors='ignore',\n",
    "        binary=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "\n",
    "# wv_data = word_sentence_list\n",
    "wv_data = word_sentence_list[0]\n",
    "\n",
    "result = pd.DataFrame()\n",
    "\n",
    "for i in range(len(wv_data)):\n",
    "\n",
    "    word_vec = {}\n",
    "    for w in word_sentence_list[i]:\n",
    "        try:\n",
    "            word_vec[w] = model.get_vector(w)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Create DF\n",
    "    keys = list(word_vec.keys())\n",
    "    vec_df = pd.DataFrame({'word':keys,\n",
    "                          'index':range(len(keys))})\n",
    "\n",
    "    vec_df = cbyz.df_cross_join(vec_df, vec_df)\n",
    "    vec_df = vec_df[vec_df['index_x']<vec_df['index_y']] \\\n",
    "            .reset_index(drop=True) \n",
    "    \n",
    "    for j in range(len(vec_df)):\n",
    "        vec1 = model.get_vector(vec_df.loc[j, 'word_x'])\n",
    "        vec2 = model.get_vector(vec_df.loc[j, 'word_y'])\n",
    "        similarity = np.dot(matutils.unitvec(vec1), matutils.unitvec(vec2))\n",
    "        vec_df.loc[j, 'similarity'] = similarity\n",
    "\n",
    "    result = result.append(vec_df)\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print(i, '/', len(word_sentence_list))\n",
    "    \n",
    "result = result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "\n",
    "wv_data = word_sentence_list\n",
    "# wv_data = word_sentence_list[0]\n",
    "\n",
    "result = pd.DataFrame()\n",
    "\n",
    "for i in range(len(wv_data)):\n",
    "\n",
    "    word_vec = {}\n",
    "    for w in word_sentence_list[i]:\n",
    "        try:\n",
    "            word_vec[w] = model.get_vector(w)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Create DF\n",
    "    keys = list(word_vec.keys())\n",
    "    vec_df = pd.DataFrame({'word':keys,\n",
    "                          'index':range(len(keys))})\n",
    "\n",
    "    for j in range(1, len(vec_df)):\n",
    "        vec1 = model.get_vector(vec_df.loc[j, 'word'])\n",
    "        vec2 = model.get_vector(vec_df.loc[j-1, 'word'])\n",
    "        similarity = np.dot(matutils.unitvec(vec1), matutils.unitvec(vec2))\n",
    "        vec_df.loc[j, 'similarity'] = similarity\n",
    "\n",
    "    result = result.append(vec_df)\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print(i, '/', len(word_sentence_list))\n",
    "    \n",
    "result = result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sentence_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_excel(path_export + '/wv_result.xlsx', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "寶格麗\n",
    "馨香\n",
    "浪漫\n",
    "玫香\n",
    "淡香精\n",
    "\n",
    "bvlgari\n",
    "寶格麗\n",
    "晶澈\n",
    "女性\n",
    "淡香水\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[result['similarity']>0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.vectors\n",
    "# model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - 跟淡香水之間的距離\n",
    "#   和男性、女性、中性的距離\n",
    "# - 如果title中同時出現「淡香水」、「淡香精」就排除"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
